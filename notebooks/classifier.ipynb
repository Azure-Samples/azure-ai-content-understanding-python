{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Content Understanding - Classifier and Analyzer Demo\n",
    "\n",
    "This notebook demonstrates how to use Azure AI Content Understanding service to:\n",
    "1. Create a classifier to categorize documents\n",
    "2. Create a custom analyzer to extract specific fields\n",
    "3. Combine classifier and analyzers to classify, optionally split, and analyze documents in a flexible processing pipeline\n",
    "\n",
    "If you‚Äôd like to learn more before getting started, see the official documentation:\n",
    "[Understanding Classifiers in Azure AI Services](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/concepts/classifier)\n",
    "\n",
    "## Prerequisites\n",
    "1. Ensure Azure AI service is configured following [steps](../README.md#configure-azure-ai-service-resource)\n",
    "2. Install the required packages to run the sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Azure Content Understanding Client\n",
    "\n",
    "The `AzureContentUnderstandingClient` class handles all API interactions with the Azure AI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "try:\n",
    "    from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "    print(\"‚úÖ Azure Content Understanding Client imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Error: Make sure 'AzureContentUnderstandingClient.py' is in the same directory as this notebook.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Azure AI Service Settings and Prepare the Sample\n",
    "\n",
    "Update these settings to match your Azure environment:\n",
    "\n",
    "- **AZURE_AI_ENDPOINT**: Your Azure AI service endpoint URL or set up in \".env\" file\n",
    "- **AZURE_AI_API_VERSION**: The Azure AI API version to use. Default is \"2025-05-01-preview\". \n",
    "- **AZURE_AI_API_KEY**: Your Azure AI service key (optional if using token authentication)\n",
    "- **ANALYZER_SAMPLE_FILE**: Path to the PDF document you want to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For authentication, you can use either token-based auth or subscription key, and only one of them is required\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set up in \".env\" file if not using token auth\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "AZURE_AI_API_VERSION = os.getenv(\"AZURE_AI_API_VERSION\", \"2025-05-01-preview\")\n",
    "ANALYZER_SAMPLE_FILE = \"../data/mixed_financial_docs.pdf\"  # Update this path to your PDF file\n",
    "\n",
    "# Authentication - Using DefaultAzureCredential for token-based auth\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "file_location = Path(ANALYZER_SAMPLE_FILE)\n",
    "\n",
    "print(\"üìã Configuration Summary:\")\n",
    "print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "print(f\"   API Version: {AZURE_AI_API_VERSION}\")\n",
    "print(f\"   Document: {file_location.name if file_location.exists() else '‚ùå File not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Classifier Schema\n",
    "\n",
    "The classifier schema defines:\n",
    "- **Categories**: Document types to classify (e.g., Legal, Medical)\n",
    "  - **description (Optional)**: An optional field used to provide additional context or hints for categorizing or splitting documents. This can be helpful when the category name alone isn‚Äôt descriptive enough. If the category name is already clear and self-explanatory, this field can be omitted.\n",
    "- **splitMode Options**: Defines how multi-page documents should be split before classification or analysis.\n",
    "  - `\"auto\"`: Automatically split based on content.  \n",
    "  For example, if two categories are defined as ‚Äúinvoice‚Äù and ‚Äúapplication form‚Äù:\n",
    "    - A PDF with only one invoice will be classified as a single document.\n",
    "    - A PDF containing two invoices and one application form will be automatically split into three classified sections.\n",
    "  - `\"none\"`: No splitting.  \n",
    "  The entire multi-page document is treated as a single unit for classification and analysis.\n",
    "  - `\"perPage\"`: Split by page.  \n",
    "  Each page is treated as a separate document. This is useful when you‚Äôve built custom analyzers designed to operate on a per-page basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define document categories and their descriptions\n",
    "classifier_schema = {\n",
    "    \"categories\": {\n",
    "        \"Loan application\": {  # Both space and underscore are allowed in category names\n",
    "            \"description\": \"Documents submitted by individuals or businesses to request funding, typically including personal or business details, financial history, loan amount, purpose, and supporting documentation.\"\n",
    "        },\n",
    "        \"Invoice\": {\n",
    "            \"description\": \"Billing documents issued by sellers or service providers to request payment for goods or services, detailing items, prices, taxes, totals, and payment terms.\"\n",
    "        },\n",
    "        \"Bank_Statement\": {  # Both space and underscore are allowed in category names\n",
    "            \"description\": \"Official statements issued by banks that summarize account activity over a period, including deposits, withdrawals, fees, and balances.\"\n",
    "        },\n",
    "    },\n",
    "    \"splitMode\": \"auto\"  # IMPORTANT: Automatically detect document boundaries. Can change mode for your needs.\n",
    "}\n",
    "\n",
    "print(\"üìÑ Classifier Categories:\")\n",
    "for category, details in classifier_schema[\"categories\"].items():\n",
    "    print(f\"   ‚Ä¢ {category}: {details['description'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Content Understanding Client\n",
    "\n",
    "Create the client that will communicate with Azure AI services.\n",
    "\n",
    "‚ö†Ô∏è Important:\n",
    "You must update the code below to match your Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments and modify those sections accordingly.\n",
    "If you skip this step, the sample may not run correctly.\n",
    "\n",
    "‚ö†Ô∏è Note: Using a subscription key works, but using a token provider with Azure Active Directory (AAD) is much safer and is highly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure Content Understanding client\n",
    "try:\n",
    "    content_understanding_client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=AZURE_AI_API_VERSION,\n",
    "        # IMPORTANT: Comment out token_provider if using subscription key\n",
    "        token_provider=token_provider,\n",
    "        # IMPORTANT: Uncomment this if using subscription key\n",
    "        # subscription_key=AZURE_AI_API_KEY,\n",
    "    )\n",
    "    print(\"‚úÖ Content Understanding client initialized successfully!\")\n",
    "    print(\"   Ready to create classifiers and analyzers.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize client: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Basic Classifier\n",
    "\n",
    "First, we'll create a simple classifier that categorizes documents without additional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique classifier ID\n",
    "classifier_id = \"classifier-sample-\" + str(uuid.uuid4())\n",
    "\n",
    "try:\n",
    "    # Create classifier\n",
    "    print(f\"üî® Creating classifier: {classifier_id}\")\n",
    "    print(\"   This may take a few seconds...\")\n",
    "    \n",
    "    response = content_understanding_client.begin_create_classifier(classifier_id, classifier_schema)\n",
    "    result = content_understanding_client.poll_result(response)\n",
    "    \n",
    "    print(\"\\n‚úÖ Classifier created successfully!\")\n",
    "    print(f\"   Status: {result.get('status')}\")\n",
    "    print(f\"   Resource Location: {result.get('resourceLocation')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error creating classifier: {e}\")\n",
    "    if \"already exists\" in str(e):\n",
    "        print(\"\\nüí° Tip: The classifier already exists. You can:\")\n",
    "        print(\"   1. Use a different classifier ID\")\n",
    "        print(\"   2. Delete the existing classifier first\")\n",
    "        print(\"   3. Skip to document classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classify Your Document\n",
    "\n",
    "Now let's use the classifier to categorize your document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if document exists\n",
    "    if not file_location.exists():\n",
    "        raise FileNotFoundError(f\"Document not found at {file_location}\")\n",
    "    \n",
    "    # Classify document\n",
    "    print(f\"üìÑ Classifying document: {file_location.name}\")\n",
    "    print(\"\\n‚è≥ Processing... This may take a few minutes for large documents.\")\n",
    "    \n",
    "    response = content_understanding_client.begin_classify(classifier_id, file_location=str(file_location))\n",
    "    result = content_understanding_client.poll_result(response, timeout_seconds=360)\n",
    "    \n",
    "    print(\"\\n‚úÖ Classification completed successfully!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå Document not found: {file_location}\")\n",
    "    print(\"   Please update file_location to point to your PDF file.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error classifying document: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Classification Results\n",
    "\n",
    "Let's examine what the classifier found in your document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification results\n",
    "if 'result' in locals() and result:\n",
    "    result_data = result.get(\"result\", {})\n",
    "    contents = result_data.get(\"contents\", [])\n",
    "    \n",
    "    print(\"üìä CLASSIFICATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nTotal sections found: {len(contents)}\")\n",
    "    \n",
    "    # Show summary of each classified section\n",
    "    print(\"\\nüìã Document Sections:\")\n",
    "    for i, content in enumerate(contents, 1):\n",
    "        print(f\"\\n   Section {i}:\")\n",
    "        print(f\"   ‚Ä¢ Category: {content.get('category', 'Unknown')}\")\n",
    "        print(f\"   ‚Ä¢ Pages: {content.get('startPageNumber', '?')} - {content.get('endPageNumber', '?')}\")\n",
    "        \n",
    "    print(\"\\nFull result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(\"‚ùå No results available. Please run the classification step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create a Custom Analyzer (Advanced)\n",
    "\n",
    "Now let's create a custom analyzer that can extract specific fields from documents.\n",
    "This analyzer will:\n",
    "- Extract common fields from loan application documents\n",
    "- Generate document excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define analyzer schema with custom fields\n",
    "analyzer_schema = {\n",
    "    \"description\": \"Loan application analyzer - extracts key information from loan application\",\n",
    "    \"baseAnalyzerId\": \"prebuilt-documentAnalyzer\",  # Built on top of the general document analyzer\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"enableLayout\": True,          # Extract layout information\n",
    "        \"enableBarcode\": False,        # Skip barcode detection\n",
    "        \"enableFormula\": False,        # Skip formula detection\n",
    "        \"estimateFieldSourceAndConfidence\": True, # Set to True if you want to estimate the field location (aka grounding) and confidence\n",
    "        \"disableContentFiltering\": False,\n",
    "    },\n",
    "    \"fieldSchema\": {\n",
    "        \"fields\": {\n",
    "            \"ApplicationDate\": {\n",
    "                \"type\": \"date\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The date when the loan application was submitted.\"\n",
    "            },\n",
    "            \"ApplicantName\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The full name of the loan applicant or company.\"\n",
    "            },\n",
    "            \"LoanAmountRequested\": {\n",
    "                \"type\": \"number\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The total amount of loan money requested by the applicant.\"\n",
    "            },\n",
    "            \"LoanPurpose\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The stated purpose or reason for the loan.\"\n",
    "            },\n",
    "            \"CreditScore\": {\n",
    "                \"type\": \"number\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The credit score of the applicant, if available.\"\n",
    "            },\n",
    "            \"Summary\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"A brief overview of the loan application details.\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate unique analyzer ID\n",
    "analyzer_id = \"analyzer-loan-application-\" + str(uuid.uuid4())\n",
    "\n",
    "# Create the analyzer\n",
    "try:\n",
    "    print(f\"üî® Creating custom analyzer: {analyzer_id}\")\n",
    "    print(\"\\nüìã Analyzer will extract:\")\n",
    "    for field_name, field_info in analyzer_schema[\"fieldSchema\"][\"fields\"].items():\n",
    "        print(f\"   ‚Ä¢ {field_name}: {field_info['description']}\")\n",
    "    \n",
    "    response = content_understanding_client.begin_create_analyzer(analyzer_id, analyzer_schema)\n",
    "    result = content_understanding_client.poll_result(response)\n",
    "    \n",
    "    print(\"\\n‚úÖ Analyzer created successfully!\")\n",
    "    print(f\"   Analyzer ID: {analyzer_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error creating analyzer: {e}\")\n",
    "    analyzer_id = None  # Set to None if creation failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create an Enhanced Classifier with Custom Analyzer\n",
    "\n",
    "Now we'll create a new classifier that uses the prebuilt invoice analyzer for invoices and our custom analyzer for loan application documents.\n",
    "This combines classification with field extraction in one operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique enhanced classifier ID\n",
    "enhanced_classifier_id = \"classifier-enhanced-\" + str(uuid.uuid4())\n",
    "\n",
    "# Create enhanced classifier schema\n",
    "enhanced_classifier_schema = {\n",
    "    \"categories\": {\n",
    "        \"Loan application\": {  # Both space and underscore are allowed in category names\n",
    "            \"description\": \"Documents submitted by individuals or businesses to request funding, typically including personal or business details, financial history, loan amount, purpose, and supporting documentation.\",\n",
    "            \"analyzerId\": analyzer_id  # IMPORTANT: Use created custom analyzer in previous step for loan applications\n",
    "        },\n",
    "        \"Invoice\": {\n",
    "            \"description\": \"Billing documents issued by sellers or service providers to request payment for goods or services, detailing items, prices, taxes, totals, and payment terms.\",\n",
    "            \"analyzerId\": \"prebuilt-invoice\"  # Use prebuilt invoice analyzer for invoices\n",
    "        },\n",
    "        \"Bank_Statement\": {  # Both space and underscore are allowed in category names\n",
    "            \"description\": \"Official statements issued by banks that summarize account activity over a period, including deposits, withdrawals, fees, and balances.\"\n",
    "            # No analyzer specified - uses default processing\n",
    "        }\n",
    "    },\n",
    "    \"splitMode\": \"auto\"\n",
    "}\n",
    "\n",
    "# Create the enhanced classifier\n",
    "if analyzer_id:  # Only create if analyzer was successfully created\n",
    "    try:\n",
    "        print(f\"üî® Creating enhanced classifier: {enhanced_classifier_id}\")\n",
    "        print(\"\\nüìã Configuration:\")\n",
    "        print(\"   ‚Ä¢ Loan application documents ‚Üí Custom analyzer with field extraction\")\n",
    "        print(\"   ‚Ä¢ Invoice documents ‚Üí Prebuilt invoice analyzer\")\n",
    "        print(\"   ‚Ä¢ Bank_Statement documents ‚Üí Standard processing\")\n",
    "        \n",
    "        response = content_understanding_client.begin_create_classifier(enhanced_classifier_id, enhanced_classifier_schema)\n",
    "        result = content_understanding_client.poll_result(response)\n",
    "        \n",
    "        print(\"\\n‚úÖ Enhanced classifier created successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error creating enhanced classifier: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping enhanced classifier creation - analyzer was not created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Process Document with Enhanced Classifier\n",
    "\n",
    "Let's process the document again using our enhanced classifier.\n",
    "Invoices and loan application documents will now have additional fields extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'enhanced_classifier_id' in locals() and analyzer_id:\n",
    "    try:\n",
    "        # Check if document exists\n",
    "        if not file_location.exists():\n",
    "            raise FileNotFoundError(f\"Document not found at {file_location}\")\n",
    "        \n",
    "        # Process with enhanced classifier\n",
    "        print(\"üìÑ Processing document with enhanced classifier\")\n",
    "        print(f\"   Document: {file_location.name}\")\n",
    "        print(\"\\n‚è≥ Processing with classification + field extraction...\")\n",
    "        \n",
    "        response = content_understanding_client.begin_classify(enhanced_classifier_id, file_location=str(file_location))\n",
    "        enhanced_result = content_understanding_client.poll_result(response, timeout_seconds=360)\n",
    "        \n",
    "        print(\"\\n‚úÖ Enhanced processing completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing document: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping enhanced classification - enhanced classifier was not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. View Enhanced Results with Extracted Fields\n",
    "\n",
    "Let's see the classification results along with the extracted fields from loan application documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display enhanced results\n",
    "if 'enhanced_result' in locals() and enhanced_result:\n",
    "    result_data = enhanced_result.get(\"result\", {})\n",
    "    contents = result_data.get(\"contents\", [])\n",
    "    \n",
    "    print(\"üìä ENHANCED CLASSIFICATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTotal sections found: {len(contents)}\")\n",
    "    \n",
    "    # Process each section\n",
    "    for i, content in enumerate(contents, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"SECTION {i}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        category = content.get('category', 'Unknown')\n",
    "        print(f\"\\nüìÅ Category: {category}\")\n",
    "        print(f\"üìÑ Pages: {content.get('startPageNumber', '?')} - {content.get('endPageNumber', '?')}\")\n",
    "        \n",
    "        # Show extracted fields from field extraction\n",
    "        fields = content.get('fields', {})\n",
    "        if fields:\n",
    "            print(\"\\nüîç Extracted Information:\")\n",
    "            for field_name, field_data in fields.items():\n",
    "                print(f\"\\n   {field_name}:\")\n",
    "                print(f\"   ‚Ä¢ Value: {field_data}\")\n",
    "else:\n",
    "    print(\"‚ùå No enhanced results available. Please run the enhanced classification step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see the fulll JSON result below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(enhanced_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "1. ‚úÖ Created a basic classifier to categorize documents\n",
    "2. ‚úÖ Created a custom analyzer to extract specific fields\n",
    "3. ‚úÖ Combined them into an enhanced classifier for intelligent document processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
