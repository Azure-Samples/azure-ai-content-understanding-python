{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Content Understanding - Classifier and Analyzer Demo\n",
    "\n",
    "This notebook demonstrates how to use Azure AI Content Understanding service to:\n",
    "1. Create a classifier to categorize documents\n",
    "2. Create a custom analyzer to extract specific fields\n",
    "3. Combine classifier and analyzer for intelligent document processing\n",
    "\n",
    "## Prerequisites\n",
    "- Azure subscription with access to Azure AI services\n",
    "- Python 3.8 or higher\n",
    "- A PDF document for testing (sample included)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Azure Content Understanding Client\n",
    "\n",
    "The `AzureContentUnderstandingClient` class handles all API interactions with the Azure AI service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "try:\n",
    "    from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "    print(\"‚úÖ Azure Content Understanding Client imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Error: Make sure 'AzureContentUnderstandingClient.py' is in the same directory as this notebook.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Azure AI Service Settings\n",
    "\n",
    "Update these settings to match your Azure environment:\n",
    "\n",
    "- **azure_ai_service_endpoint**: Your Azure AI service endpoint URL\n",
    "- **subscription_key**: Your subscription key (optional if using token authentication)\n",
    "- **file_location**: Path to the PDF document you want to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "AZURE_AI_API_VERSION = os.getenv(\"AZURE_AI_API_VERSION\", \"2025-05-01-preview\")\n",
    "ANALYZER_SAMPLE_FILE = '../data/MS_Annual_Report_2024.pdf' # Update this path to your PDF file\n",
    "\n",
    "file_location = Path(ANALYZER_SAMPLE_FILE)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key, and only one of them is required\n",
    "\n",
    "# Authentication - Using DefaultAzureCredential for token-based auth\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "# IMPORTANT: Replace with your actual subscription key if not using token auth\n",
    "subscription_key = \"dummy_key\"\n",
    "\n",
    "print(\"üìã Configuration Summary:\")\n",
    "print(f\"   Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "print(f\"   API Version: {AZURE_AI_API_VERSION}\")\n",
    "print(f\"   Document: {file_location.name if file_location.exists() else '‚ùå File not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Classifier Schema\n",
    "\n",
    "The classifier schema defines:\n",
    "- **Categories**: Document types to classify (e.g., Legal, Medical)\n",
    "- **Split Mode**: How to split multi-page documents\n",
    "  - `\"auto\"`: Automatically split based on content\n",
    "  - `\"none\"`: Don't split\n",
    "  - `\"perPage\"`: Split every page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define document categories and their descriptions\n",
    "classifier_schema = {\n",
    "    \"categories\": {\n",
    "        \"Executive Summary & Strategy\": {\n",
    "            \"description\": \"Leadership messages, strategic vision, and future outlook.\"\n",
    "        },\n",
    "        \"Financial Performance & Segment Reporting\": {\n",
    "            \"description\": \"Overall financial results and detailed performance by business units.\"\n",
    "        },\n",
    "        \"Operations & Corporate Governance\": {\n",
    "            \"description\": \"Business operations, governance structure, and risk management.\"\n",
    "        },\n",
    "        \"Shareholder Information & Relations\": {\n",
    "            \"description\": \"Annual meeting details, stock information, and shareholder services.\"\n",
    "        }\n",
    "    },\n",
    "    \"splitMode\": \"auto\"  # IMPORTANT: Automatically detect document boundaries\n",
    "}\n",
    "\n",
    "print(\"üìÑ Classifier Categories:\")\n",
    "for category, details in classifier_schema[\"categories\"].items():\n",
    "    print(f\"   ‚Ä¢ {category}: {details['description'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Content Understanding Client\n",
    "\n",
    "Create the client that will communicate with Azure AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Azure Content Understanding client\n",
    "try:\n",
    "    content_understanding_client = AzureContentUnderstandingClient(\n",
    "        endpoint=AZURE_AI_ENDPOINT,\n",
    "        api_version=AZURE_AI_API_VERSION,\n",
    "        # IMPORTANT: Comment out token_provider if using subscription key\n",
    "        token_provider=token_provider,\n",
    "        # IMPORTANT: Uncomment this if using subscription key\n",
    "        # subscription_key=subscription_key\n",
    "    )\n",
    "    print(\"‚úÖ Content Understanding client initialized successfully!\")\n",
    "    print(\"   Ready to create classifiers and analyzers.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize client: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Basic Classifier\n",
    "\n",
    "First, we'll create a simple classifier that categorizes documents without additional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique classifier ID\n",
    "classifier_id = \"classifier-sample-\" + str(uuid.uuid4())\n",
    "\n",
    "try:\n",
    "    # Create classifier\n",
    "    print(f\"üî® Creating classifier: {classifier_id}\")\n",
    "    print(\"   This may take a few seconds...\")\n",
    "    \n",
    "    response = content_understanding_client.begin_create_classifier(classifier_id, classifier_schema)\n",
    "    result = content_understanding_client.poll_result(response)\n",
    "    \n",
    "    print(\"\\n‚úÖ Classifier created successfully!\")\n",
    "    print(f\"   Status: {result.get('status')}\")\n",
    "    print(f\"   Resource Location: {result.get('resourceLocation')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error creating classifier: {e}\")\n",
    "    if \"already exists\" in str(e):\n",
    "        print(\"\\nüí° Tip: The classifier already exists. You can:\")\n",
    "        print(\"   1. Use a different classifier ID\")\n",
    "        print(\"   2. Delete the existing classifier first\")\n",
    "        print(\"   3. Skip to document classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classify Your Document\n",
    "\n",
    "Now let's use the classifier to categorize your document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Check if document exists\n",
    "    if not file_location.exists():\n",
    "        raise FileNotFoundError(f\"Document not found at {file_location}\")\n",
    "    \n",
    "    # Classify document\n",
    "    print(f\"üìÑ Classifying document: {file_location.name}\")\n",
    "    print(\"\\n‚è≥ Processing... This may take a few minutes for large documents.\")\n",
    "    \n",
    "    response = content_understanding_client.begin_classify(classifier_id, file_location=str(file_location))\n",
    "    result = content_understanding_client.poll_result(response, timeout_seconds=360)\n",
    "    \n",
    "    print(\"\\n‚úÖ Classification completed successfully!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå Document not found: {file_location}\")\n",
    "    print(\"   Please update file_location to point to your PDF file.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error classifying document: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Classification Results\n",
    "\n",
    "Let's examine what the classifier found in your document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification results\n",
    "if 'result' in locals() and result:\n",
    "    result_data = result.get(\"result\", {})\n",
    "    contents = result_data.get(\"contents\", [])\n",
    "    \n",
    "    print(\"üìä CLASSIFICATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nTotal sections found: {len(contents)}\")\n",
    "    \n",
    "    # Show summary of each classified section\n",
    "    print(\"\\nüìã Document Sections:\")\n",
    "    for i, content in enumerate(contents, 1):\n",
    "        print(f\"\\n   Section {i}:\")\n",
    "        print(f\"   ‚Ä¢ Category: {content.get('category', 'Unknown')}\")\n",
    "        print(f\"   ‚Ä¢ Pages: {content.get('startPageNumber', '?')} - {content.get('endPageNumber', '?')}\")\n",
    "        \n",
    "    print(\"\\nFull result:\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "else:\n",
    "    print(\"‚ùå No results available. Please run the classification step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create a Custom Analyzer (Advanced)\n",
    "\n",
    "Now let's create a custom analyzer that can extract specific fields from documents.\n",
    "This analyzer will:\n",
    "- Extract visit dates from medical documents\n",
    "- Generate document excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define analyzer schema with custom fields\n",
    "analyzer_schema = {\n",
    "    \"description\": \"Medical encounter analyzer - extracts key information from medical records\",\n",
    "    \"baseAnalyzerId\": \"prebuilt-documentAnalyzer\",  # Built on top of the general document analyzer\n",
    "    \"config\": {\n",
    "        \"returnDetails\": True,\n",
    "        \"enableLayout\": True,          # Extract layout information\n",
    "        \"enableBarcode\": False,        # Skip barcode detection\n",
    "        \"enableFormula\": False,        # Skip formula detection\n",
    "        \"estimateFieldSourceAndConfidence\": False, # Set to True if you want to estimate the field location (aka grounding) and confidence\n",
    "        \"disableContentFiltering\": False,\n",
    "    },\n",
    "    \"fieldSchema\": {\n",
    "        \"fields\": {\n",
    "            \"ReportDate\": {\n",
    "                \"type\": \"date\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The publication or filing date of the annual report.\"\n",
    "            },\n",
    "            \"CompanyName\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The name of the company issuing the report.\"\n",
    "            },\n",
    "            \"FiscalYear\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"The fiscal year the report covers.\"\n",
    "            },\n",
    "            \"NetIncome\": {\n",
    "                \"type\": \"number\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"Net income or profit reported for the fiscal year.\"\n",
    "            },\n",
    "            \"Summary\": {\n",
    "                \"type\": \"string\",\n",
    "                \"method\": \"generate\",\n",
    "                \"description\": \"Brief summary of the annual report\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate unique analyzer ID\n",
    "analyzer_id = \"analyzer-medical-\" + str(uuid.uuid4())\n",
    "\n",
    "# Create the analyzer\n",
    "try:\n",
    "    print(f\"üî® Creating custom analyzer: {analyzer_id}\")\n",
    "    print(\"\\nüìã Analyzer will extract:\")\n",
    "    for field_name, field_info in analyzer_schema[\"fieldSchema\"][\"fields\"].items():\n",
    "        print(f\"   ‚Ä¢ {field_name}: {field_info['description']}\")\n",
    "    \n",
    "    response = content_understanding_client.begin_create_analyzer(analyzer_id, analyzer_schema)\n",
    "    result = content_understanding_client.poll_result(response)\n",
    "    \n",
    "    print(\"\\n‚úÖ Analyzer created successfully!\")\n",
    "    print(f\"   Analyzer ID: {analyzer_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error creating analyzer: {e}\")\n",
    "    analyzer_id = None  # Set to None if creation failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create an Enhanced Classifier with Custom Analyzer\n",
    "\n",
    "Now we'll create a new classifier that uses our custom analyzer for medical documents.\n",
    "This combines classification with field extraction in one operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique enhanced classifier ID\n",
    "enhanced_classifier_id = \"classifier-enhanced-\" + str(uuid.uuid4())\n",
    "\n",
    "# Create enhanced classifier schema\n",
    "enhanced_classifier_schema = {\n",
    "    \"categories\": {\n",
    "        \"Legal\": {\n",
    "            \"description\": \"Legal documents including subpoenas, declarations, contracts, and other legal paperwork.\"\n",
    "            # No analyzer specified - uses default processing\n",
    "        },\n",
    "        \"Annual Report\": {\n",
    "            \"description\": \"Each document must ends with 'end of encounter'. Dont rely on page numbers\",\n",
    "            \"analyzerId\": analyzer_id  # IMPORTANT: Use our custom analyzer for annual reports\n",
    "        },\n",
    "        \"Declaration_of_custodian\": {\n",
    "            \"description\": \"Declarations of custodian documents, often used in legal contexts.\"\n",
    "        }\n",
    "    },\n",
    "    \"splitMode\": \"auto\"\n",
    "}\n",
    "\n",
    "# Create the enhanced classifier\n",
    "if analyzer_id:  # Only create if analyzer was successfully created\n",
    "    try:\n",
    "        print(f\"üî® Creating enhanced classifier: {enhanced_classifier_id}\")\n",
    "        print(\"\\nüìã Configuration:\")\n",
    "        print(\"   ‚Ä¢ Legal documents ‚Üí Standard processing\")\n",
    "        print(\"   ‚Ä¢ Medical documents ‚Üí Custom analyzer with field extraction\")\n",
    "        \n",
    "        response = content_understanding_client.begin_create_classifier(enhanced_classifier_id, enhanced_classifier_schema)\n",
    "        result = content_understanding_client.poll_result(response)\n",
    "        \n",
    "        print(\"\\n‚úÖ Enhanced classifier created successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error creating enhanced classifier: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping enhanced classifier creation - analyzer was not created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Process Document with Enhanced Classifier\n",
    "\n",
    "Let's process the document again using our enhanced classifier.\n",
    "Medical documents will now have additional fields extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'enhanced_classifier_id' in locals() and analyzer_id:\n",
    "    try:\n",
    "        # Check if document exists\n",
    "        if not file_location.exists():\n",
    "            raise FileNotFoundError(f\"Document not found at {file_location}\")\n",
    "        \n",
    "        # Process with enhanced classifier\n",
    "        print(\"üìÑ Processing document with enhanced classifier\")\n",
    "        print(f\"   Document: {file_location.name}\")\n",
    "        print(\"\\n‚è≥ Processing with classification + field extraction...\")\n",
    "        \n",
    "        response = content_understanding_client.begin_classify(enhanced_classifier_id, file_location=str(file_location))\n",
    "        enhanced_result = content_understanding_client.poll_result(response, timeout_seconds=360)\n",
    "        \n",
    "        print(\"\\n‚úÖ Enhanced processing completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing document: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping enhanced classification - enhanced classifier was not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. View Enhanced Results with Extracted Fields\n",
    "\n",
    "Let's see the classification results along with the extracted fields from medical documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display enhanced results\n",
    "if 'enhanced_result' in locals() and enhanced_result:\n",
    "    result_data = enhanced_result.get(\"result\", {})\n",
    "    contents = result_data.get(\"contents\", [])\n",
    "    \n",
    "    print(\"üìä ENHANCED CLASSIFICATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTotal sections found: {len(contents)}\")\n",
    "    \n",
    "    # Process each section\n",
    "    for i, content in enumerate(contents, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"SECTION {i}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        category = content.get('category', 'Unknown')\n",
    "        print(f\"\\nüìÅ Category: {category}\")\n",
    "        print(f\"üìÑ Pages: {content.get('startPageNumber', '?')} - {content.get('endPageNumber', '?')}\")\n",
    "        \n",
    "        # Show extracted fields for medical documents\n",
    "        fields = content.get('fields', {})\n",
    "        if fields:\n",
    "            print(\"\\nüîç Extracted Information:\")\n",
    "            for field_name, field_data in fields.items():\n",
    "                print(f\"\\n   {field_name}:\")\n",
    "                print(f\"   ‚Ä¢ Value: {field_data}\")\n",
    "else:\n",
    "    print(\"‚ùå No enhanced results available. Please run the enhanced classification step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see the fulll JSON result below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(enhanced_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "1. ‚úÖ Created a basic classifier to categorize documents\n",
    "2. ‚úÖ Created a custom analyzer to extract specific fields\n",
    "3. ‚úÖ Combined them into an enhanced classifier for intelligent document processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
