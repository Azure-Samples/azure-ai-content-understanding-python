{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhance Your Analyzer with Labeled Data\n",
    "\n",
    "> #################################################################################\n",
    ">\n",
    "> Note: Currently, this feature is available only when the analyzer scenario is set to `document`.\n",
    ">\n",
    "> #################################################################################\n",
    "\n",
    "Labeled data consists of samples that have been tagged with one or more labels to provide context or meaning. This labeling is used to improve the analyzer's performance.\n",
    "\n",
    "In your own projects, you can use [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/quickstart/use-ai-foundry) to annotate your data with the labeling tool.\n",
    "\n",
    "This notebook demonstrates how to create an analyzer using labeled data and how to analyze your files after the labeled data is prepared.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Follow the steps in [Set environment variables for training data](../docs/set_env_for_training_data_and_reference_doc.md) to add the training data related environment variables `TRAINING_DATA_SAS_URL` and `TRAINING_DATA_PATH` into the [.env](./.env) file:\n",
    "    - `TRAINING_DATA_SAS_URL`: SAS URL for your Azure Blob container.\n",
    "    - `TRAINING_DATA_PATH`: Folder path within the container where training data will be uploaded.\n",
    "3. Install the necessary packages to run the sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer Template and Local Training Folder Setup\n",
    "In this sample, we define a template for receipts.\n",
    "\n",
    "The training folder should contain a flat (single-level) directory of labeled receipt documents. Each document should include:\n",
    "- The original file (e.g., PDF or image).\n",
    "- A corresponding `labels.json` file containing labeled fields.\n",
    "- A corresponding `result.json` file containing OCR results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_template = \"../analyzer_templates/receipt.json\"\n",
    "training_docs_folder = \"../data/document_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure Content Understanding Client\n",
    "\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class that encapsulates necessary functions. Before the official release of the Content Understanding SDK, consider this a lightweight SDK.\n",
    "\n",
    "Fill in the constants **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, and **AZURE_AI_API_KEY** with your Azure AI Service details.\n",
    "\n",
    "> ⚠️ Important:\n",
    ">\n",
    "You must update the code below to match your Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments and modify those sections accordingly.\n",
    "If you omit this step, the sample may not run correctly.\n",
    "\n",
    "> ⚠️ Note: Using a subscription key works, but using an Azure Active Directory (AAD) token provider is much safer and is highly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Import utility package from python samples root directory\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = AzureContentUnderstandingClient(\n",
    "    endpoint=os.getenv(\"AZURE_AI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_AI_API_VERSION\", \"2025-05-01-preview\"),\n",
    "    # IMPORTANT: Comment out token_provider if using subscription key\n",
    "    token_provider=token_provider,\n",
    "    # IMPORTANT: Uncomment this if using subscription key\n",
    "    # subscription_key=os.getenv(\"AZURE_AI_API_KEY\"),\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/analyzer_training\",  # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Labeled Data\n",
    "\n",
    "In this step, we will:\n",
    "- Verify that each document file in the local folder has corresponding `.labels.json` and `.result.json` files.\n",
    "- Upload these files to the configured Azure Blob storage.\n",
    "\n",
    "This process uses the **TRAINING_DATA_SAS_URL** and **TRAINING_DATA_PATH** environment variables set up during the Prerequisites step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_SAS_URL = os.getenv(\"TRAINING_DATA_SAS_URL\")\n",
    "TRAINING_DATA_PATH = os.getenv(\"TRAINING_DATA_PATH\")\n",
    "\n",
    "await client.generate_training_data_on_blob(training_docs_folder, TRAINING_DATA_SAS_URL, TRAINING_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Analyzer with Defined Schema\n",
    "\n",
    "Before creating the analyzer, set the constant `ANALYZER_ID` to a meaningful name for your task. Here, we generate a unique suffix so that this cell can be run multiple times to create different analyzers.\n",
    "\n",
    "This step uses the **TRAINING_DATA_SAS_URL** and **TRAINING_DATA_PATH** variables configured in the [.env](./.env) file and referenced in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "CUSTOM_ANALYZER_ID = \"train-sample-\" + str(uuid.uuid4())\n",
    "\n",
    "response = client.begin_create_analyzer(\n",
    "    CUSTOM_ANALYZER_ID,\n",
    "    analyzer_template_path=analyzer_template,\n",
    "    training_storage_container_sas_url=TRAINING_DATA_SAS_URL,\n",
    "    training_storage_container_path_prefix=TRAINING_DATA_PATH,\n",
    ")\n",
    "result = client.poll_result(response)\n",
    "if result is not None and \"status\" in result and result[\"status\"] == \"Succeeded\":\n",
    "    logging.info(f\"Analyzer details for {result['result']['analyzerId']}\")\n",
    "    logging.info(json.dumps(result, indent=2))\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"An issue was encountered while creating the analyzer. \"\n",
    "        \"Please double-check your deployment and configurations for potential issues.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Created Analyzer to Extract Document Content\n",
    "\n",
    "Once the analyzer is successfully created, you can use it to analyze your input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.begin_analyze(CUSTOM_ANALYZER_ID, file_location='../data/receipt.png')\n",
    "result_json = client.poll_result(response)\n",
    "\n",
    "logging.info(json.dumps(result_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Existing Analyzer in Content Understanding Service\n",
    "\n",
    "This step is optional but recommended to prevent test analyzers from accumulating in your service. Without deletion, the analyzer will remain available for subsequent reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_analyzer(CUSTOM_ANALYZER_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}