{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhance Your Analyzer with Labeled Data\n",
    "\n",
    "\n",
    "> #################################################################################\n",
    ">\n",
    "> Note: Currently, this feature is only available when the analyzer scenario is set to `document`.\n",
    ">\n",
    "> #################################################################################\n",
    "\n",
    "Labeled data consists of samples that have been tagged with one or more labels to add context or meaning. This additional information is used to improve the analyzer's performance.\n",
    "\n",
    "In your own projects, you can use [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/quickstart/use-ai-foundry) to annotate your data with the labeling tool.\n",
    "\n",
    "This notebook demonstrates how to create an analyzer using your labeled data and how to analyze your files afterward.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. Set environment variables related to training data by following the steps in [Set env for training data](../docs/set_env_for_training_data_and_reference_doc.md) and adding them to the [.env](./.env) file.\n",
    "   - You can either set `TRAINING_DATA_SAS_URL` directly with the SAS URL for your Azure Blob container,\n",
    "   - Or set both `TRAINING_DATA_STORAGE_ACCOUNT_NAME` and `TRAINING_DATA_CONTAINER_NAME` to generate the SAS URL automatically during later steps.\n",
    "   - Also set `TRAINING_DATA_PATH` to specify the folder path within the container where the training data will be uploaded.\n",
    "3. Install the packages required to run the sample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer Template and Local Training Folder Setup\n",
    "In this sample, we define a template for receipts.\n",
    "\n",
    "The training folder should contain a flat (one-level) directory of labeled receipt documents. Each document includes:\n",
    "- The original file (e.g., PDF or image).\n",
    "- A corresponding `labels.json` file with labeled fields.\n",
    "- A corresponding `result.json` file with OCR results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_docs_folder = \"../data/document_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure Content Understanding Client\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class that contains helper functions. Before the official release of the Content Understanding SDK, please consider it a lightweight SDK.\n",
    ">\n",
    "> Fill in the constants **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, and **AZURE_AI_API_KEY** with the information from your Azure AI Service.\n",
    "\n",
    "> ‚ö†Ô∏è Important:\n",
    "You must update the code below to match your Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments and modify those sections accordingly.\n",
    "If you skip this step, the sample may not run correctly.\n",
    "\n",
    "> ‚ö†Ô∏è Note: While using a subscription key works, using a token provider with Azure Active Directory (AAD) is safer and highly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from azure.storage.blob import ContainerSasPermissions\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.contentunderstanding.aio import ContentUnderstandingClient\n",
    "from azure.ai.contentunderstanding.models import (\n",
    "    ContentAnalyzer,\n",
    "    FieldSchema,\n",
    "    FieldDefinition,\n",
    "    FieldType,\n",
    "    GenerationMethod,\n",
    "    AnalysisMode,\n",
    "    ProcessingLocation,\n",
    ")\n",
    "\n",
    "# Add the parent directory to the Python path to import the sample_helper module\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'python'))\n",
    "from extension.document_processor import DocumentProcessor\n",
    "from extension.sample_helper import extract_operation_id_from_poller, PollerType, save_json_to_file\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "endpoint = os.environ.get(\"AZURE_CONTENT_UNDERSTANDING_ENDPOINT\")\n",
    "# Return AzureKeyCredential if AZURE_CONTENT_UNDERSTANDING_KEY is set, otherwise DefaultAzureCredential\n",
    "key = os.getenv(\"AZURE_CONTENT_UNDERSTANDING_KEY\")\n",
    "credential = AzureKeyCredential(key) if key else DefaultAzureCredential()\n",
    "# Create the ContentUnderstandingClient\n",
    "client = ContentUnderstandingClient(endpoint=endpoint, credential=credential)\n",
    "print(\"‚úÖ ContentUnderstandingClient created successfully\")\n",
    "\n",
    "try:\n",
    "    processor = DocumentProcessor(client)\n",
    "    print(\"‚úÖ DocumentProcessor created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create DocumentProcessor: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Labeled Data\n",
    "In this step, we will:\n",
    "- Use the environment variables `TRAINING_DATA_PATH` and SAS URL related variables set in the Prerequisites step.\n",
    "- Attempt to get the SAS URL from the environment variable `TRAINING_DATA_SAS_URL`.\n",
    "- If `TRAINING_DATA_SAS_URL` is not set, try generating it automatically using `TRAINING_DATA_STORAGE_ACCOUNT_NAME` and `TRAINING_DATA_CONTAINER_NAME` environment variables.\n",
    "- Verify that each document file in the local folder has corresponding `.labels.json` and `.result.json` files.\n",
    "- Upload these files to the Azure Blob storage container specified by the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference storage configuration from environment\n",
    "training_data_path = os.getenv(\"TRAINING_DATA_PATH\") or f\"training_data_{uuid.uuid4().hex[:8]}\"\n",
    "training_data_sas_url = os.getenv(\"TRAINING_DATA_SAS_URL\")\n",
    "\n",
    "if not training_data_path.endswith(\"/\"):\n",
    "    training_data_path += \"/\"\n",
    "\n",
    "if not training_data_sas_url:\n",
    "    TRAINING_DATA_STORAGE_ACCOUNT_NAME = os.getenv(\"TRAINING_DATA_STORAGE_ACCOUNT_NAME\")\n",
    "    TRAINING_DATA_CONTAINER_NAME = os.getenv(\"TRAINING_DATA_CONTAINER_NAME\")\n",
    "    print(f\"TRAINING_DATA_STORAGE_ACCOUNT_NAME: {TRAINING_DATA_STORAGE_ACCOUNT_NAME}\")\n",
    "    print(f\"TRAINING_DATA_CONTAINER_NAME: {TRAINING_DATA_CONTAINER_NAME}\")\n",
    "\n",
    "    if TRAINING_DATA_STORAGE_ACCOUNT_NAME and TRAINING_DATA_CONTAINER_NAME:\n",
    "        # We require \"Write\" permission to upload, modify, or append blobs\n",
    "        training_data_sas_url = processor.generate_container_sas_url(\n",
    "            account_name=TRAINING_DATA_STORAGE_ACCOUNT_NAME,\n",
    "            container_name=TRAINING_DATA_CONTAINER_NAME,\n",
    "            permissions=ContainerSasPermissions(read=True, write=True, list=True),\n",
    "            expiry_hours=1,\n",
    "        )\n",
    "\n",
    "await processor.generate_training_data_on_blob(training_docs_folder, training_data_sas_url, training_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Analyzer with Defined Schema\n",
    "Before creating the analyzer, fill in the constant `ANALYZER_ID` with a relevant name for your task. In this example, we generate a unique suffix so that this cell can be run multiple times to create different analyzers.\n",
    "\n",
    "We use **TRAINING_DATA_SAS_URL** and **TRAINING_DATA_PATH** as set in the [.env](./.env) file and used in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "analyzer_id = f\"analyzer-training-sample-{datetime.now().strftime('%Y%m%d')}-{datetime.now().strftime('%H%M%S')}-{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "content_analyzer = ContentAnalyzer(\n",
    "    base_analyzer_id=\"prebuilt-documentAnalyzer\",\n",
    "    description=\"Extract useful information from receipt\",\n",
    "    field_schema=FieldSchema(\n",
    "        name=\"receipt schema\",\n",
    "        description=\"Schema for receipt\",\n",
    "        fields={\n",
    "            \"MerchantName\": FieldDefinition(\n",
    "                type=FieldType.STRING,\n",
    "                method=GenerationMethod.EXTRACT,\n",
    "                description=\"\"\n",
    "            ),\n",
    "            \"Items\": FieldDefinition(\n",
    "                type=FieldType.ARRAY,\n",
    "                method=GenerationMethod.GENERATE,\n",
    "                description=\"\",\n",
    "                items_property={\n",
    "                    \"type\": \"object\",\n",
    "                    \"method\": \"extract\",\n",
    "                    \"properties\": {\n",
    "                        \"Quantity\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"method\": \"extract\",\n",
    "                            \"description\": \"\"\n",
    "                        },\n",
    "                        \"Name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"method\": \"extract\",\n",
    "                            \"description\": \"\"\n",
    "                        },\n",
    "                        \"Price\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"method\": \"extract\",\n",
    "                            \"description\": \"\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            \"TotalPrice\": FieldDefinition(\n",
    "                type=FieldType.STRING,\n",
    "                method=GenerationMethod.EXTRACT,\n",
    "                description=\"\"\n",
    "            )\n",
    "        }\n",
    "    ),\n",
    "    mode=AnalysisMode.STANDARD,\n",
    "    processing_location=ProcessingLocation.GEOGRAPHY,\n",
    "    tags={\"demo_type\": \"get_result\"},\n",
    "    training_data={\n",
    "        \"kind\": \"blob\",\n",
    "        \"containerUrl\": training_data_sas_url,\n",
    "        \"prefix\": training_data_path\n",
    "    },\n",
    ")\n",
    "print(f\"üîß Creating custom analyzer '{analyzer_id}'...\")\n",
    "poller = await client.content_analyzers.begin_create_or_replace(\n",
    "    analyzer_id=analyzer_id,\n",
    "    resource=content_analyzer,\n",
    ")\n",
    "\n",
    "# Extract operation ID from the poller\n",
    "operation_id = extract_operation_id_from_poller(\n",
    "    poller, PollerType.ANALYZER_CREATION\n",
    ")\n",
    "print(f\"üìã Extracted creation operation ID: {operation_id}\")\n",
    "\n",
    "# Wait for the analyzer to be created\n",
    "print(f\"‚è≥ Waiting for analyzer creation to complete...\")\n",
    "await poller.result()\n",
    "print(f\"‚úÖ Analyzer '{analyzer_id}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Created Analyzer to Extract Document Content\n",
    "After the analyzer is successfully created, you can use it to analyze your input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/receipt.png\"\n",
    "print(f\"üìÑ Reading document file: {file_path}\")\n",
    "with open(file_path, \"rb\") as f:\n",
    "    data_content = f.read()\n",
    "\n",
    "# Begin document analysis operation\n",
    "print(f\"üîç Starting document analysis with analyzer '{analyzer_id}'...\")\n",
    "analysis_poller = await client.content_analyzers.begin_analyze_binary(\n",
    "    analyzer_id=analyzer_id, \n",
    "    input=data_content,\n",
    "    content_type=\"application/octet-stream\")\n",
    "\n",
    "# Wait for analysis completion\n",
    "print(f\"‚è≥ Waiting for document analysis to complete...\")\n",
    "analysis_result = await analysis_poller.result()\n",
    "print(f\"‚úÖ Document analysis completed successfully!\")\n",
    "\n",
    " # Extract operation ID for get_result\n",
    "analysis_operation_id = extract_operation_id_from_poller(\n",
    "    analysis_poller, PollerType.ANALYZE_CALL\n",
    ")\n",
    "print(f\"üìã Extracted analysis operation ID: {analysis_operation_id}\")\n",
    "\n",
    "# Get the analysis result using the operation ID\n",
    "print(\n",
    "    f\"üîç Getting analysis result using operation ID '{analysis_operation_id}'...\"\n",
    ")\n",
    "operation_status = await client.content_analyzers.get_result(\n",
    "    operation_id=analysis_operation_id,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Analysis result retrieved successfully!\")\n",
    "print(f\"   Operation ID: {operation_status.id}\")\n",
    "print(f\"   Status: {operation_status.status}\")\n",
    "\n",
    "# The actual analysis result is in operation_status.result\n",
    "operation_result = operation_status.result\n",
    "if operation_result is None:\n",
    "    print(\"‚ö†Ô∏è  No analysis result available\")\n",
    "\n",
    "print(f\"üìÑ Analysis Result: {json.dumps(operation_result.as_dict())}\")\n",
    "\n",
    "# Save the analysis result to a file\n",
    "saved_file_path = save_json_to_file(\n",
    "    result=operation_result.as_dict(),\n",
    "    filename_prefix=\"analyzer_training_get_result\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Existing Analyzer in Content Understanding Service\n",
    "This snippet is optional and is included to prevent test analyzers from remaining in your service. Without deletion, the analyzer will stay in your service and may be reused in subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üóëÔ∏è  Deleting analyzer '{analyzer_id}' (demo cleanup)...\")\n",
    "await client.content_analyzers.delete(analyzer_id=analyzer_id)\n",
    "print(f\"‚úÖ Analyzer '{analyzer_id}' deleted successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
