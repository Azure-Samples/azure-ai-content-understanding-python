{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhance your analyzer with labeled data\n",
    "\n",
    "\n",
    "> #################################################################################\n",
    ">\n",
    "> Note: Currently this feature is only available for analyzer scenario is `document`\n",
    ">\n",
    "> #################################################################################\n",
    "\n",
    "Labeled data is a group of samples that have been tagged with one or more labels to add context or meaning, which is used to improve analyzer's performance.\n",
    "\n",
    "In your own project, you will use [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/quickstart/use-ai-foundry) to use the labeling tool to annotate your data.\n",
    "\n",
    "In this notebook we will demonstrate after you have the labeled data, how to create analyzer with them and analyze your files.\n",
    "\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "1. Ensure Azure AI service is configured following [steps](../README.md#configure-azure-ai-service-resource)\n",
    "1. Follow steps in [Set labeled data](../docs/set_env_for_labeled_data.md) to add training data related env variables in `.env`.\n",
    "1. Install packages needed to run the sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer template and local training folder set up\n",
    "In this sample we define a template for receipts.\n",
    "\n",
    "The training folder should contain a flat (one-level) directory of labeled receipt documents. Each document includes:\n",
    "- The original file (e.g., PDF or image).\n",
    "- A corresponding labels.json file with labeled fields.\n",
    "- A corresponding result.json file with OCR results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_template = \"../analyzer_templates/receipt.json\"\n",
    "training_docs_folder = \"../data/document_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure content understanding client\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is utility class that contains the functions, Before the release of the Content Understanding SDK, please consider it a lightweight SDK., Fill in values for the constants **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, **AZURE_AI_API_KEY** with the information from your Azure AI Service.\n",
    "\n",
    "> ⚠️ Important:\n",
    "You must update the code below to match your Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments and modify those sections accordingly.\n",
    "If you skip this step, the sample may not run correctly.\n",
    "\n",
    "> ⚠️ Note: Using a subscription key works, but using a token provider with Azure Active Directory (AAD) is much safer and is highly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# import utility package from python samples root directory\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = AzureContentUnderstandingClient(\n",
    "    endpoint=os.getenv(\"AZURE_AI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_AI_API_VERSION\", \"2025-05-01-preview\"),\n",
    "    # IMPORTANT: Comment out token_provider if using subscription key\n",
    "    token_provider=token_provider,\n",
    "    # IMPORTANT: Uncomment this if using subscription key\n",
    "    # subscription_key=os.getenv(\"AZURE_AI_API_KEY\"),\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/analyzer_training\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare labeled data\n",
    "In this step, we will \n",
    "- Check whether document files in local folder have corresponding `.labels.json` and `.result.json` files\n",
    "- Upload these files to the designated Azure blob storage.\n",
    "\n",
    "We use **TRAINING_DATA_SAS_URL** and **TRAINING_DATA_PATH** that's set in the Prerequisites step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_SAS_URL = os.getenv(\"TRAINING_DATA_SAS_URL\")\n",
    "TRAINING_DATA_PATH = os.getenv(\"TRAINING_DATA_PATH\")\n",
    "\n",
    "await client.generate_training_data_on_blob(training_docs_folder, TRAINING_DATA_SAS_URL, TRAINING_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create analyzer with defined schema\n",
    "Before creating the analyzer, you should fill in the constant ANALYZER_ID with a relevant name to your task. Here, we generate a unique suffix so this cell can be run multiple times to create different analyzers.\n",
    "\n",
    "We use **TRAINING_DATA_SAS_URL** and **TRAINING_DATA_PATH** that's set up in the `.env` file and used in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "CUSTOM_ANALYZER_ID = \"train-sample-\" + str(uuid.uuid4())\n",
    "\n",
    "response = client.begin_create_analyzer(\n",
    "    CUSTOM_ANALYZER_ID,\n",
    "    analyzer_template_path=analyzer_template,\n",
    "    training_storage_container_sas_url=TRAINING_DATA_SAS_URL,\n",
    "    training_storage_container_path_prefix=TRAINING_DATA_PATH,\n",
    ")\n",
    "result = client.poll_result(response)\n",
    "if result is not None and \"status\" in result and result[\"status\"] == \"Succeeded\":\n",
    "    logging.info(f\"Analyzer details for {result['result']['analyzerId']}\")\n",
    "    logging.info(json.dumps(result, indent=2))\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"An issue was encountered when trying to create the analyzer. \"\n",
    "        \"Please double-check your deployment and configurations for potential problems.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use created analyzer to extract document content\n",
    "After the analyzer is successfully created, we can use it to analyze our input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.begin_analyze(CUSTOM_ANALYZER_ID, file_location='../data/receipt.png')\n",
    "result_json = client.poll_result(response)\n",
    "\n",
    "logging.info(json.dumps(result_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete exist analyzer in Content Understanding Service\n",
    "This snippet is not required, but it's only used to prevent the testing analyzer from residing in your service. Without deletion, the analyzer will remain in your service for subsequent reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_analyzer(CUSTOM_ANALYZER_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
