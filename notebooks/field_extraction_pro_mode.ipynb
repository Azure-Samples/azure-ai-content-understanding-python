{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Complex Analysis with Pro mode\n",
    "\n",
    "> #################################################################################\n",
    ">\n",
    "> **Note:** Pro mode is currently available only for `document` data.  \n",
    "> [Supported file types](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/service-limits#document-and-text): pdf, tiff, jpg, jpeg, png, bmp, heif\n",
    ">\n",
    "> #################################################################################\n",
    "\n",
    "This notebook demonstrates how to use **Pro mode** in Azure AI Content Understanding to enhance your analyzer with multiple inputs and optional reference data. Pro mode is designed for advanced use cases, particularly those requiring multi-step reasoning and complex decision-making (for example, identifying inconsistencies, drawing inferences, and making sophisticated decisions). Pro mode enables input from multiple content files and includes the option to provide reference data at analyzer creation time.\n",
    "\n",
    "In this walkthrough, you will learn how to:\n",
    "1. Create an analyzer with a schema and reference data.\n",
    "2. Analyze your files using Pro mode.\n",
    "\n",
    "For more details on Pro mode, see the [Azure AI Content Understanding: Standard and Pro Modes](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/concepts/standard-pro-modes) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure the Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. If you plan to use reference documents, follow the [Set env for reference doc](../docs/set_env_for_training_data_and_reference_doc.md) instructions to set `REFERENCE_DOC_SAS_URL` and `REFERENCE_DOC_PATH` in the [.env](./.env) file.\n",
    "    - `REFERENCE_DOC_SAS_URL`: SAS URL for your Azure Blob container.\n",
    "    - `REFERENCE_DOC_PATH`: Folder path within the container for uploading reference documents.\n",
    "    > ⚠️ Note: Reference documents are optional in Pro mode. You can run Pro mode using only input documents. For example, the service can reason across two or more input files even without any reference data.\n",
    "3. Install the required packages to run the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer Template and Local Files Setup\n",
    "- **analyzer_template**: In this sample, we define an analyzer template for invoice-contract verification.\n",
    "- **input_docs**: You can provide multiple input document files in one folder or specify a single document file location.\n",
    "- **reference_docs (Optional)**: During analyzer creation, you can provide documents that aid in providing context for the analyzer at inference time. The service will generate OCR results for these files if needed, produce a reference `.jsonl` file, and upload these files to a specified Azure Blob storage location.\n",
    "\n",
    "> For example, if you want to analyze invoices for consistency with contractual agreements, you can supply the invoice and other relevant documents (e.g., purchase orders) as inputs, and supply contract files as reference data. The service applies reasoning to validate the input documents according to your schema, such as identifying discrepancies to flag for further review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for analyzer template, input documents, and reference documents\n",
    "analyzer_template = \"../analyzer_templates/invoice_contract_verification_pro_mode.json\"\n",
    "input_docs = \"../data/field_extraction_pro_mode/invoice_contract_verification/input_docs\"\n",
    "\n",
    "# NOTE: Reference documents are optional in Pro mode. Comment out the below line if not using reference documents.\n",
    "reference_docs = \"../data/field_extraction_pro_mode/invoice_contract_verification/reference_docs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's examine the analyzer template used in Pro mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(analyzer_template, \"r\") as file:\n",
    "    print(json.dumps(json.load(file), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the analyzer, the `\"mode\"` must be set to `\"pro\"`. The defined field `\"PaymentTermsInconsistencies\"` is a `\"generate\"` field, which is designed to reason about inconsistencies. It will utilize the reference documents uploaded to the [reference docs](../data/field_extraction_pro_mode/invoice_contract_verification/reference_docs) folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure Content Understanding Client\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class containing the necessary functions. Note that before the release of the Content Understanding SDK, consider it a lightweight SDK.\n",
    "Fill in the values for the constants **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, **AZURE_AI_API_KEY** with the information from your Azure AI Service.\n",
    "\n",
    "> ⚠️ Important:\n",
    "You must update the code below to match your Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments and modify those sections accordingly.\n",
    "If you skip this step, the sample may not run correctly.\n",
    "\n",
    "> ⚠️ Note: While using a subscription key works, using a token provider with Azure Active Directory (AAD) is more secure and strongly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Import utility package from python samples root directory\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key; only one is required\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set up in the \".env\" file if not using token auth\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "AZURE_AI_API_VERSION = os.getenv(\"AZURE_AI_API_VERSION\", \"2025-05-01-preview\")\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_ENDPOINT,\n",
    "    api_version=AZURE_AI_API_VERSION,\n",
    "    # IMPORTANT: Comment out token_provider if using subscription key\n",
    "    token_provider=token_provider,\n",
    "    # IMPORTANT: Uncomment this if using subscription key\n",
    "    # subscription_key=AZURE_AI_API_KEY,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/pro_mode\",  # This header is used for sample usage telemetry; comment out if opting out.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Reference Data\n",
    "In this step, we will:\n",
    "- Use the Azure AI service to extract OCR results from reference documents (if needed).\n",
    "- Generate a reference `.jsonl` file.\n",
    "- Upload these files to the designated Azure Blob storage.\n",
    "\n",
    "We utilize **REFERENCE_DOC_SAS_URL** and **REFERENCE_DOC_PATH**, which are set in the Prerequisites step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference storage configuration from environment\n",
    "REFERENCE_DOC_SAS_URL = os.getenv(\"REFERENCE_DOC_SAS_URL\")\n",
    "REFERENCE_DOC_PATH = os.getenv(\"REFERENCE_DOC_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ Note: Reference documents are optional in Pro mode. You can run Pro mode using only input documents. The service can reason across two or more input files even without any reference data. To skip preparation of reference documents, comment out or omit the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set skip_analyze to True if you already have OCR results for the documents in the reference_docs folder\n",
    "# Ensure OCR result files are named with the original document file name including its extension plus the suffix \".result.json\"\n",
    "# For example, for \"invoice.pdf\", the OCR result should be named \"invoice.pdf.result.json\"\n",
    "# NOTE: Comment out the following line if you do not have any reference documents.\n",
    "await client.generate_knowledge_base_on_blob(reference_docs, REFERENCE_DOC_SAS_URL, REFERENCE_DOC_PATH, skip_analyze=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Analyzer with Defined Schema for Pro mode\n",
    "Before creating the analyzer, assign a relevant name to the constant `ANALYZER_ID`. Here, we generate a unique suffix so this cell can be executed multiple times to create different analyzers.\n",
    "\n",
    "We use **REFERENCE_DOC_SAS_URL** and **REFERENCE_DOC_PATH** configured in the [.env](./.env) file and utilized in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "CUSTOM_ANALYZER_ID = \"pro-mode-sample-\" + str(uuid.uuid4())\n",
    "\n",
    "response = client.begin_create_analyzer(\n",
    "    CUSTOM_ANALYZER_ID,\n",
    "    analyzer_template_path=analyzer_template,\n",
    "    pro_mode_reference_docs_storage_container_sas_url=REFERENCE_DOC_SAS_URL,\n",
    "    pro_mode_reference_docs_storage_container_path_prefix=REFERENCE_DOC_PATH,\n",
    ")\n",
    "result = client.poll_result(response)\n",
    "if result is not None and \"status\" in result and result[\"status\"] == \"Succeeded\":\n",
    "    logging.info(f\"Analyzer details for {result['result']['analyzerId']}\")\n",
    "    logging.info(json.dumps(result, indent=2))\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"An issue was encountered when creating the analyzer. \"\n",
    "        \"Please verify your deployment and configuration for potential issues.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Created Analyzer to Analyze the Input Documents\n",
    "After the analyzer is created successfully, it can be used to analyze your input files.\n",
    "> NOTE: Pro mode performs multi-step reasoning and may require a longer analysis time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, display\n",
    "\n",
    "response = client.begin_analyze(CUSTOM_ANALYZER_ID, file_location=input_docs)\n",
    "result_json = client.poll_result(response, timeout_seconds=600)  # Extended timeout for Pro mode\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, f\"{CUSTOM_ANALYZER_ID}_result.json\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(result_json, file, indent=2)\n",
    "\n",
    "logging.info(f\"Full analyzer result saved to: {output_path}\")\n",
    "display(FileLink(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's review the extracted fields produced by Pro mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = result_json[\"result\"][\"contents\"][0][\"fields\"]\n",
    "print(json.dumps(fields, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As shown in the `PaymentTermsInconsistencies` field, the purchase contract includes detailed payment terms agreed upon before the service. However, the invoice contains implied payment terms that conflict with the contract. Pro mode was able to identify the corresponding contract for this invoice from the reference documents and analyze both together to reveal this inconsistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Existing Analyzer in Content Understanding Service\n",
    "This step is optional but recommended; it prevents test analyzers from remaining in your service. Without deletion, the analyzer will persist and could be used in subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_analyzer(CUSTOM_ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Sample\n",
    "Here we present another example highlighting how Pro mode supports multi-document input and advanced reasoning.\n",
    "Unlike Document Standard Mode, which processes one document at a time, Pro mode can analyze multiple documents within a single analysis call. Pro mode not only processes each document independently, but also cross-references them to perform reasoning across documents, enabling deeper insights and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, Set Up Variables for the Second Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for analyzer template, input documents, and reference documents for the second sample\n",
    "analyzer_template_2 = \"../analyzer_templates/insurance_claims_review_pro_mode.json\"\n",
    "input_docs_2 = \"../data/field_extraction_pro_mode/insurance_claims_review/input_docs\"\n",
    "reference_docs_2 = \"../data/field_extraction_pro_mode/insurance_claims_review/reference_docs\"\n",
    "\n",
    "# Load reference storage configuration from environment\n",
    "REFERENCE_DOC_SAS_URL_2 = os.getenv(\"REFERENCE_DOC_SAS_URL\")  # Reusing the same blob container\n",
    "REFERENCE_DOC_PATH_2 = os.getenv(\"REFERENCE_DOC_PATH\").rstrip(\"/\") + \"_2/\"  # NOTE: Use a different path for the second sample\n",
    "CUSTOM_ANALYZER_ID_2 = \"pro-mode-sample-\" + str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Knowledge Base for the Second Sample\n",
    "We will upload [reference documents](../data/field_extraction_pro_mode/insurance_claims_review/reference_docs/) with existing OCR results for the second sample. These documents contain driver coverage policies useful for reviewing insurance claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Start generating knowledge base for the second sample...\")\n",
    "await client.generate_knowledge_base_on_blob(reference_docs_2, REFERENCE_DOC_SAS_URL_2, REFERENCE_DOC_PATH_2, skip_analyze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Analyzer for the Second Sample\n",
    "We will reuse the existing AzureContentUnderstandingClient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.begin_create_analyzer(\n",
    "    CUSTOM_ANALYZER_ID_2,\n",
    "    analyzer_template_path=analyzer_template_2,\n",
    "    pro_mode_reference_docs_storage_container_sas_url=REFERENCE_DOC_SAS_URL_2,\n",
    "    pro_mode_reference_docs_storage_container_path_prefix=REFERENCE_DOC_PATH_2,\n",
    ")\n",
    "result = client.poll_result(response)\n",
    "if result is not None and \"status\" in result and result[\"status\"] == \"Succeeded\":\n",
    "    logging.info(f\"Analyzer details for {result['result']['analyzerId']}\")\n",
    "    logging.info(json.dumps(result, indent=2))\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"An issue occurred while creating the analyzer. \"\n",
    "        \"Please verify your deployment and configurations for potential issues.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Multiple Input Documents Using the Second Analyzer\n",
    "Note that the [input_docs_2](../data/field_extraction_pro_mode/insurance_claims_review/input_docs/) directory contains two PDF files as input: one is a car accident report, and the other is a repair estimate.\n",
    "\n",
    "The first document includes details such as the car’s license plate number, vehicle model, and other incident-related information.\n",
    "The second document provides a breakdown of the estimated repair costs.\n",
    "\n",
    "Due to the complexity of this multi-document scenario and the processing involved, it may take a few minutes to generate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Start analyzing input documents for the second sample...\")\n",
    "response = client.begin_analyze(CUSTOM_ANALYZER_ID_2, file_location=input_docs_2)\n",
    "result_json = client.poll_result(response, timeout_seconds=600)  # Extended timeout for Pro mode\n",
    "\n",
    "# Save the results to a JSON file\n",
    "# Ensure output directory exists\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, f\"{CUSTOM_ANALYZER_ID_2}_result.json\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(result_json, file, indent=2)\n",
    "\n",
    "logging.info(f\"Full analyzer result saved to: {output_path}\")\n",
    "display(FileLink(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the Analysis Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_json[\"result\"][\"contents\"][0][\"fields\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the `LineItemCorroboration` Field in Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The field `ReportingOfficer` appears only in the car accident report, while fields such as `VIN` come exclusively from the repair estimate document. This illustrates that information is extracted from both documents to produce a single unified result, demonstrating an N:1 relationship between inputs and the analysis output.\n",
    "\n",
    "> Multiple input documents are combined to generate one comprehensive output. This is not a batch model where N input documents yield N outputs; instead, all inputs are reasoned about jointly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = result_json[\"result\"][\"contents\"][0][\"fields\"][\"LineItemCorroboration\"]\n",
    "print(json.dumps(fields, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `LineItemCorroboration` field shows that each line item, extracted from the *repair estimate document*, includes corresponding information, claim status, and evidence.\n",
    "> Items not covered by the policy, such as a Starbucks drink and hotel stay, are flagged as suspicious, while repair damages supported by supplied claim documents and permitted by the policy are confirmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Delete the Analyzer for the Second Sample After Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_analyzer(CUSTOM_ANALYZER_ID_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}