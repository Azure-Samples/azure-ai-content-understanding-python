{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Complex Analysis with Pro Mode\n",
    "\n",
    "> #################################################################################\n",
    ">\n",
    "> **Note:** Pro mode is currently available only for `document` data.  \n",
    "> [Supported file types](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/service-limits#document-and-text): pdf, tiff, jpg, jpeg, png, bmp, heif\n",
    ">\n",
    "> #################################################################################\n",
    "\n",
    "This notebook demonstrates how to use **Pro mode** in Azure AI Content Understanding to enhance your analyzer with multiple inputs and optional reference data. Pro mode is designed for advanced use cases that require multi-step reasoning and complex decision-making, such as identifying inconsistencies, drawing inferences, and making sophisticated decisions. Pro mode enables input from multiple content files and allows you to provide reference data at analyzer creation time.\n",
    "\n",
    "In this walkthrough, you'll learn how to:\n",
    "1. Create an analyzer using a schema and reference data.\n",
    "2. Analyze your files using Pro mode.\n",
    "\n",
    "For more details on Pro mode, see the [Azure AI Content Understanding: Standard and Pro Modes](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/concepts/standard-pro-modes) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Ensure your Azure AI service is configured by following the [configuration steps](../README.md#configure-azure-ai-service-resource).\n",
    "2. If using reference documents, set up `REFERENCE_DOC_SAS_URL` and `REFERENCE_DOC_PATH` in the [.env](./.env) file by following the instructions in [Set env for reference doc](../docs/set_env_for_training_data_and_reference_doc.md).\n",
    "    - `REFERENCE_DOC_SAS_URL`: SAS URL for your Azure Blob container.\n",
    "    - `REFERENCE_DOC_PATH`: Folder path within the container for uploading reference documents.\n",
    "    > ⚠️ Note: Reference documents are optional in Pro mode. You can run Pro mode using only input documents. For example, the service can reason across two or more input files without any reference data.\n",
    "3. Install the required packages to run the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer Template and Local File Setup\n",
    "- **analyzer_template**: In this sample, we define an analyzer template for invoice-contract verification.\n",
    "- **input_docs**: You can provide multiple input document files in one folder or specify a single document file location.\n",
    "- **reference_docs (Optional)**: During analyzer creation, you can provide documents that serve as context for the analyzer during inference. If needed, OCR results will be generated for these files, a reference JSONL file will be produced, and these files will be uploaded to a designated Azure Blob storage container.\n",
    "\n",
    "> For example, if you want to analyze invoices to ensure consistency with a contractual agreement, you can supply the invoice and other relevant documents (such as a purchase order) as inputs, and provide the contract files as reference data. The service uses reasoning to validate the input documents against your schema, identifying discrepancies to flag for further review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for analyzer template, input documents, and reference documents\n",
    "analyzer_template = \"../analyzer_templates/invoice_contract_verification_pro_mode.json\"\n",
    "input_docs = \"../data/field_extraction_pro_mode/invoice_contract_verification/input_docs\"\n",
    "\n",
    "# NOTE: Reference documents are optional in Pro mode. Comment out the line below if not using reference documents.\n",
    "reference_docs = \"../data/field_extraction_pro_mode/invoice_contract_verification/reference_docs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's examine the analyzer template used for Pro mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(analyzer_template, \"r\") as file:\n",
    "    print(json.dumps(json.load(file), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the analyzer, the `\"mode\"` must be set to `\"pro\"`. The defined field `\"PaymentTermsInconsistencies\"` is a `\"generate\"` field designed to reason about inconsistencies. It can also utilize the referenced documents uploaded to the [reference docs](../data/field_extraction_pro_mode/invoice_contract_verification/reference_docs) folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure Content Understanding Client\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class containing relevant functions. Before the official Content Understanding SDK release, please consider it a lightweight SDK. \n",
    "Fill in the constants **AZURE_AI_ENDPOINT**, **AZURE_AI_API_VERSION**, and **AZURE_AI_API_KEY** with your Azure AI Service information.\n",
    "\n",
    "> ⚠️ Important:\n",
    "Update the code below to match your Azure authentication method. \n",
    "Look for the `# IMPORTANT` comments and adjust those sections accordingly.\n",
    "Skipping this step may cause the sample to fail.\n",
    "\n",
    "> ⚠️ Note: Using a subscription key is supported, but using a token provider with Azure Active Directory (AAD) is more secure and highly recommended for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Import utility package from python samples root directory\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# For authentication, you can use either token-based auth or subscription key; only one is required\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in the \".env\" file if not using token auth\n",
    "AZURE_AI_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "AZURE_AI_API_VERSION = os.getenv(\"AZURE_AI_API_VERSION\", \"2025-05-01-preview\")\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_ENDPOINT,\n",
    "    api_version=AZURE_AI_API_VERSION,\n",
    "    # IMPORTANT: Comment out token_provider if using subscription key\n",
    "    token_provider=token_provider,\n",
    "    # IMPORTANT: Uncomment this line if using subscription key\n",
    "    # subscription_key=AZURE_AI_API_KEY,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/pro_mode\",  # Used for sample usage telemetry; comment out to opt out.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Reference Data\n",
    "In this step, we will:\n",
    "- Use Azure AI service to extract OCR results from reference documents (if needed).\n",
    "- Generate a reference `.jsonl` file.\n",
    "- Upload these files to the designated Azure Blob storage.\n",
    "\n",
    "This process uses **REFERENCE_DOC_SAS_URL** and **REFERENCE_DOC_PATH** set during the Prerequisites step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference storage configuration from environment\n",
    "REFERENCE_DOC_SAS_URL = os.getenv(\"REFERENCE_DOC_SAS_URL\")\n",
    "REFERENCE_DOC_PATH = os.getenv(\"REFERENCE_DOC_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ Note: Reference documents are optional in Pro mode. You can run Pro mode using only input documents. For example, the service can reason across two or more input files without any reference data. \n",
    "> If you do not have reference documents, please skip or comment out the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set skip_analyze to True if you already have OCR results for the documents in the reference_docs folder.\n",
    "# OCR result files must be named with the original document file name plus the suffix \".result.json\".\n",
    "# For example, if the original file is \"invoice.pdf\", the OCR result should be named \"invoice.pdf.result.json\".\n",
    "# NOTE: Comment out this line if you do not have reference documents.\n",
    "await client.generate_knowledge_base_on_blob(reference_docs, REFERENCE_DOC_SAS_URL, REFERENCE_DOC_PATH, skip_analyze=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Analyzer with Defined Schema for Pro Mode\n",
    "Before creating the analyzer, assign a relevant name to the constant `ANALYZER_ID`. Here, we generate a unique suffix so this cell can be run multiple times to create different analyzers.\n",
    "\n",
    "We use **REFERENCE_DOC_SAS_URL** and **REFERENCE_DOC_PATH** set in the [.env](./.env) file, as used in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "CUSTOM_ANALYZER_ID = \"pro-mode-sample-\" + str(uuid.uuid4())\n",
    "\n",
    "response = client.begin_create_analyzer(\n",
    "    CUSTOM_ANALYZER_ID,\n",
    "    analyzer_template_path=analyzer_template,\n",
    "    pro_mode_reference_docs_storage_container_sas_url=REFERENCE_DOC_SAS_URL,\n",
    "    pro_mode_reference_docs_storage_container_path_prefix=REFERENCE_DOC_PATH,\n",
    ")\n",
    "result = client.poll_result(response)\n",
    "if result is not None and \"status\" in result and result[\"status\"] == \"Succeeded\":\n",
    "    logging.info(f\"Analyzer details for {result['result']['analyzerId']}\")\n",
    "    logging.info(json.dumps(result, indent=2))\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"An issue was encountered when trying to create the analyzer. \"\n",
    "        \"Please double-check your deployment and configurations for potential problems.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Input Documents Using the Created Analyzer\n",
    "Once the analyzer is successfully created, you can use it to analyze your input files.\n",
    "> NOTE: Pro mode involves multi-step reasoning and may take longer to complete the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, display\n",
    "\n",
    "response = client.begin_analyze(CUSTOM_ANALYZER_ID, file_location=input_docs)\n",
    "result_json = client.poll_result(response, timeout_seconds=600)  # Increased timeout for Pro mode\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, f\"{CUSTOM_ANALYZER_ID}_result.json\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(result_json, file, indent=2)\n",
    "\n",
    "logging.info(f\"Full analyzer result saved to: {output_path}\")\n",
    "display(FileLink(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's examine the extracted fields from Pro mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = result_json[\"result\"][\"contents\"][0][\"fields\"]\n",
    "print(json.dumps(fields, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For example, the field `PaymentTermsInconsistencies` shows that the purchase contract contains detailed payment terms agreed upon prior to the service. However, the implied payment terms on the invoice conflict with this. Pro mode was able to identify the corresponding contract for this invoice from the reference documents and analyze both together to uncover this inconsistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Existing Analyzer from Content Understanding Service\n",
    "This step is optional but recommended to prevent unnecessary analyzers from accumulating in your service. Without deletion, the analyzer will remain in your service and may affect subsequent usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_analyzer(CUSTOM_ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Sample\n",
    "This additional sample demonstrates how Pro mode supports multi-document input and advanced reasoning. Unlike Document Standard Mode, which processes one document at a time, Pro mode can analyze multiple documents in a single analysis call. Pro mode processes each document independently and cross-references them to perform reasoning across documents, providing deeper insights and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Variables for the Second Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for analyzer template, input documents, and reference documents for the second sample\n",
    "analyzer_template_2 = \"../analyzer_templates/insurance_claims_review_pro_mode.json\"\n",
    "input_docs_2 = \"../data/field_extraction_pro_mode/insurance_claims_review/input_docs\"\n",
    "reference_docs_2 = \"../data/field_extraction_pro_mode/insurance_claims_review/reference_docs\"\n",
    "\n",
    "# Load reference storage configuration from environment\n",
    "REFERENCE_DOC_SAS_URL_2 = os.getenv(\"REFERENCE_DOC_SAS_URL\")  # Reuse the same blob container\n",
    "REFERENCE_DOC_PATH_2 = os.getenv(\"REFERENCE_DOC_PATH\").rstrip(\"/\") + \"_2/\"  # Use a different path for the second sample\n",
    "CUSTOM_ANALYZER_ID_2 = \"pro-mode-sample-\" + str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Knowledge Base for the Second Sample\n",
    "Upload [reference documents](../data/field_extraction_pro_mode/insurance_claims_review/reference_docs/) with existing OCR results for this sample. These documents contain driver coverage policies useful for reviewing insurance claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting to generate knowledge base for the second sample...\")\n",
    "await client.generate_knowledge_base_on_blob(reference_docs_2, REFERENCE_DOC_SAS_URL_2, REFERENCE_DOC_PATH_2, skip_analyze=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Analyzer for the Second Sample\n",
    "We reuse the previous AzureContentUnderstandingClient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.begin_create_analyzer(\n",
    "    CUSTOM_ANALYZER_ID_2,\n",
    "    analyzer_template_path=analyzer_template_2,\n",
    "    pro_mode_reference_docs_storage_container_sas_url=REFERENCE_DOC_SAS_URL_2,\n",
    "    pro_mode_reference_docs_storage_container_path_prefix=REFERENCE_DOC_PATH_2,\n",
    ")\n",
    "result = client.poll_result(response)\n",
    "if result is not None and \"status\" in result and result[\"status\"] == \"Succeeded\":\n",
    "    logging.info(f\"Analyzer details for {result['result']['analyzerId']}\")\n",
    "    logging.info(json.dumps(result, indent=2))\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"An issue was encountered when trying to create the analyzer. \"\n",
    "        \"Please double-check your deployment and configurations for potential problems.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Multiple Input Documents with the Second Analyzer\n",
    "The [input_docs_2](../data/field_extraction_pro_mode/insurance_claims_review/input_docs/) directory contains two PDF files as input: a car accident report and a repair estimate.\n",
    "\n",
    "The first document includes details such as the vehicle's license plate number, vehicle model, and other incident-related information.\n",
    "The second document provides a breakdown of the estimated repair costs.\n",
    "\n",
    "Due to the complexity of this multi-document scenario and the processing involved, generating the results may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting analysis of input documents for the second sample...\")\n",
    "response = client.begin_analyze(CUSTOM_ANALYZER_ID_2, file_location=input_docs_2)\n",
    "result_json = client.poll_result(response, timeout_seconds=600)  # Increased timeout for Pro mode\n",
    "\n",
    "# Save the result to a JSON file\n",
    "# Create the output directory if it doesn't exist\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, f\"{CUSTOM_ANALYZER_ID_2}_result.json\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(result_json, file, indent=2)\n",
    "\n",
    "logging.info(f\"Full analyzer result saved to: {output_path}\")\n",
    "display(FileLink(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Analysis Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_json[\"result\"][\"contents\"][0][\"fields\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Look at the `LineItemCorroboration` Field\n",
    "\n",
    "> The field `ReportingOfficer` is present only in the car accident report, while fields like `VIN` are found only in the repair estimate document. This demonstrates that information is extracted from both documents to produce a single consolidated result. \n",
    "> This also illustrates the many-to-one relationship between input documents and the analysis output—multiple input documents contribute to one unified analysis result. This is not a batch model where each input document yields a separate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = result_json[\"result\"][\"contents\"][0][\"fields\"][\"LineItemCorroboration\"]\n",
    "print(json.dumps(fields, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the `LineItemCorroboration` field, each line item generated from the *repair estimate document* is extracted along with its corresponding information, claim status, and evidence. Items not covered by the policy, such as a Starbucks drink and hotel stay, are marked as suspicious, while damage repairs supported by the supplied claim documents and permitted by the policy are confirmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Delete the Analyzer for the Second Sample After Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_analyzer(CUSTOM_ANALYZER_ID_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}